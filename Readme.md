<div align="center">

# üé¨ AskTube

### Chat with any YouTube video using AI

[![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=flat-square&logo=python&logoColor=white)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-009688?style=flat-square&logo=fastapi&logoColor=white)](https://fastapi.tiangolo.com)
[![Docker](https://img.shields.io/badge/Docker-Ready-2496ED?style=flat-square&logo=docker&logoColor=white)](https://docker.com)
[![LangGraph](https://img.shields.io/badge/LangGraph-Agent-FF6B6B?style=flat-square)](https://github.com/langchain-ai/langgraph)
[![ChromaDB](https://img.shields.io/badge/ChromaDB-Vector%20Store-orange?style=flat-square)](https://www.trychroma.com)
[![Whisper](https://img.shields.io/badge/OpenAI-Whisper-412991?style=flat-square&logo=openai&logoColor=white)](https://github.com/openai/whisper)

</div>

---

## üìΩÔ∏è Demo

> **Click the thumbnail below to watch the full project walkthrough on Loom**

<div align="center">

![Watch the demo](https://www.loom.com/share/82c1a6fbd27b4b909d3c82cd5e03db8b)


</div>

---

## üß† How It Works

AskTube lets you paste any public YouTube URL, automatically downloads the video, transcribes it using OpenAI Whisper, indexes the transcription into ChromaDB, and exposes a real-time WebSocket chat interface powered by a LangGraph agent ‚Äî all served via a single FastAPI backend.

```
     YouTube URL
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  yt-dlp / pytube ‚îÇ  ‚îÄ‚îÄ downloads video.mp4 + audio.wav
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  OpenAI Whisper  ‚îÇ  ‚îÄ‚îÄ transcribes audio ‚Üí timestamped segments (.jsonl)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    ChromaDB      ‚îÇ  ‚îÄ‚îÄ embeds & indexes transcript chunks
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   EasyOCR        ‚îÇ  ‚îÄ‚îÄ extracts text from video frames & visual content
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LangGraph Agent ‚îÇ  ‚îÄ‚îÄ routes queries, retrieves context, calls LLM
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
   WebSocket Chat  ‚îÄ‚îÄ  real-time responses in the browser
```

---
### ü§ñ LangGraph Agent Architecture

The chatbot uses a conditional routing graph to intelligently handle different query types:

![LangGraph Architecture](architecture/output.png)

| Route | Trigger | Behaviour |
|-------|---------|-----------|
| `GeneralQuery` | Casual or off-topic questions | Answers directly without retrieval |
| `History_Only` | Follow-up questions referencing prior context | Uses conversation history only |
| `Retrieval_Required` | Video-specific questions | Fetches relevant transcript chunks from ChromaDB + OCR data before calling the LLM |

---
## üóÇÔ∏è Project Structure
```
youtube-ask-feature/
‚îÇ
‚îú‚îÄ‚îÄ graph/
‚îÇ   ‚îú‚îÄ‚îÄ graph_nodes.py      # LangGraph node definitions & chatbot entrypoint
‚îÇ   ‚îú‚îÄ‚îÄ LLM_call.py         # LLM invocation logic
‚îÇ   ‚îî‚îÄ‚îÄ state.py            # AgentState TypedDict
‚îÇ
‚îú‚îÄ‚îÄ static/                 # Static assets served by FastAPI
‚îú‚îÄ‚îÄ upload_videos/          # Runtime directory for downloaded videos (gitignored)
‚îÇ
‚îú‚îÄ‚îÄ main.py                 # FastAPI app ‚Äî all routes & WebSocket endpoint
‚îú‚îÄ‚îÄ app.py                  # YouTube download & video processing helpers
‚îú‚îÄ‚îÄ store_in_DB.py          # ChromaDB ingestion logic
‚îú‚îÄ‚îÄ OCR.py                  # OCR utilities for visual content
‚îú‚îÄ‚îÄ transcript_from_audio.py
‚îÇ
‚îú‚îÄ‚îÄ frontend2.html          # Single-file frontend (video player + chat UI)
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .env                    # API keys (never commit this)
```

---

## ‚ú® Features

- **One-click YouTube ingestion** ‚Äî paste a URL and the pipeline runs automatically
- **In-browser video playback** ‚Äî watch the video alongside your conversation
- **Real-time AI chat via WebSockets** ‚Äî no page reloads, responses stream live
- **Timestamped retrieval** ‚Äî the agent retrieves semantically relevant transcript segments
- **LangGraph multi-node agent** ‚Äî router decides between RAG retrieval and direct response
- **Persistent vector store** ‚Äî ChromaDB persists per-job so you can re-query without re-indexing
- **Fully Dockerized** ‚Äî one command to run everything

---

## üöÄ Getting Started

### Prerequisites

- [Docker](https://docs.docker.com/get-docker/) and [Docker Compose](https://docs.docker.com/compose/install/) installed
- An NVIDIA GPU is **strongly recommended** for Whisper transcription speed (CUDA 11.8+)
- A valid OpenAI (or compatible) API key

---

### Option 1 ‚Äî Docker (Recommended)

**1. Clone the repository**

```bash
git clone https://github.com/YOUR_USERNAME/youtube-ask-feature.git
cd youtube-ask-feature
```

**2. Create your `.env` file**

```bash
cp .env.example .env   # or create it manually
```

Edit `.env` and fill in your keys:

```env
OPENAI_API_KEY=sk-...
# Add any other keys your LLM_call.py or graph nodes require
```

**3. Build and run**

```bash
docker compose up --build
```

> If you don't have a `docker-compose.yml` yet, run the container directly:

```bash
# Build the image
docker build -t asktube .

# Run with GPU support
docker run --gpus all -p 8000:8000 --env-file .env asktube

# Run without GPU (Whisper will use CPU ‚Äî slower)
docker run -p 8000:8000 --env-file .env asktube
```

**4. Open the app**

```
http://localhost:8000
```

---

### Option 2 ‚Äî Local Python (without Docker)

**1. Clone and create a virtual environment**

```bash
git clone https://github.com/YOUR_USERNAME/youtube-ask-feature.git
cd youtube-ask-feature

python -m venv myenv
source myenv/bin/activate        # Windows: myenv\Scripts\activate
```

**2. Install dependencies**

```bash
pip install -r requirements.txt
```

> **CUDA users:** Make sure your PyTorch install matches your CUDA version.
> Visit [pytorch.org/get-started](https://pytorch.org/get-started/locally/) and install the matching wheel before running pip install.

**3. Set up environment variables**

```bash
cp .env.example .env
# Edit .env with your API keys
```

**4. Run the server**

```bash
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

**5. Open the app**

```
http://localhost:8000
```

---

## üîå API Reference

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Serves the frontend HTML |
| `POST` | `/upload-url?url=<yt_url>` | Downloads YouTube video, returns `job_id` |
| `GET` | `/status/{job_id}` | Poll processing status: `processing` / `ready` / `failed` |
| `POST` | `/upload/{job_id}` | Transcribe audio ‚Üí embed ‚Üí store in ChromaDB |
| `WS` | `/upload/{job_id}/query` | WebSocket chat endpoint |
| `DELETE` | `/uploads/{job_id}/delete` | Clean up all files for a job |
| `GET` | `/videos/{job_id}/video.mp4` | Serve the downloaded video file |

---

## ‚öôÔ∏è Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | Your LLM provider API key | required |
| `WHISPER_MODEL` | Whisper model size (`tiny`, `base`, `small`, `medium`, `large`) | `base` |
| `UPLOAD_DIR` | Directory for storing video/audio/transcripts | `upload_videos/` |

Change the Whisper model size in `main.py`:

```python
model = whisper.load_model("base", device='cuda')  # change "base" ‚Üí "medium" for better accuracy
```

---

## üê≥ Dockerfile Notes

The included `Dockerfile` sets up:
- Python base image with CUDA support
- `ffmpeg` for audio extraction
- All Python dependencies from `requirements.txt`
- Exposes port `8000`

If your base image doesn't include CUDA and you want GPU transcription, change the first line of your Dockerfile to:

```dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04
```

And ensure you pass `--gpus all` when running the container.

---

## üõ†Ô∏è Troubleshooting

**Video not loading after URL submission**
- Make sure the YouTube URL is public and not age-restricted
- Check `upload_videos/{job_id}/` exists and contains `video.mp4`

**Transcription is very slow**
- You're running on CPU. Pass `--gpus all` to Docker or install a CUDA-compatible PyTorch

**WebSocket disconnects immediately**
- Ensure you called `POST /upload/{job_id}` first ‚Äî chat is only available after indexing
- Check that ChromaDB directory was created at `upload_videos/{job_id}/chromaDB/`

**ChromaDB errors on startup**
- Delete stale `upload_videos/` directories from previous runs and restart

---

## üìÑ License

MIT License ‚Äî see [LICENSE](LICENSE) for details.

---

<div align="center">
Built with FastAPI ¬∑ LangGraph ¬∑ OpenAI Whisper ¬∑ ChromaDB
</div>
